# Bonnes Pratiques - Phase 9 : Bootstrap Cluster Kubernetes HA avec kubeadm  ## ðŸ“š Sources de RÃ©fÃ©rence  Documentation officielle Kubernetes : - [kubeadm HA Setup](https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/high-availability/) - [Creating a cluster with kubeadm](https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/) - [Setup HA etcd with kubeadm](https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/setup-ha-etcd-with-kubeadm/)  ## âœ… SÃ©quence d'Initialisation RecommandÃ©e  ### Ordre SÃ‰QUENTIEL (NE PAS faire en parallÃ¨le)  1. **MASTER-01** : `kubeadm init` (bootstrap du cluster)    - GÃ©nÃ¨re les certificats CA    - Initialise etcd (mode standalone)    - DÃ©marre l'API server  2. **Attendre** que master-01 soit complÃ¨tement prÃªt (API server rÃ©pond sur 6443)  3. **MASTER-02** : `kubeadm join --control-plane` (rejoint le control plane)    - RÃ©cupÃ¨re les certificats depuis master-01    - Configure etcd pour rejoindre le cluster existant    - Attend la formation du cluster etcd  4. **Attendre** que master-02 soit complÃ¨tement intÃ©grÃ© (etcd cluster formÃ©)  5. **MASTER-03** : `kubeadm join --control-plane` (rejoint le control plane)    - RÃ©cupÃ¨re les certificats depuis master-01    - Configure etcd pour rejoindre le cluster existant    - ComplÃ¨te le cluster etcd HA (3 nÅ“uds)  6. **WORKERS** : `kubeadm join` (peuvent Ãªtre joints en parallÃ¨le aprÃ¨s les masters)  ## ðŸ”§ Configuration Critique  ### 1. Utiliser un fichier de configuration kubeadm  Au lieu de flags en ligne de commande, utiliser un fichier `kubeadm-config.yaml` :  ```yaml apiVersion: kubeadm.k8s.io/v1beta4 kind: InitConfiguration localAPIEndpoint:   advertiseAddress: 10.0.0.100  # IP PRIVÃ‰E uniquement   bindPort: 6443 nodeRegistration:   criSocket: unix:///var/run/containerd/containerd.sock --- apiVersion: kubeadm.k8s.io/v1beta4 kind: ClusterConfiguration kubernetesVersion: v1.30.0 controlPlaneEndpoint: 10.0.0.100:6443  # IP PRIVÃ‰E uniquement etcd:   local:     serverCertSANs:       - 10.0.0.100  # IP PRIVÃ‰ES       - 10.0.0.101       - 10.0.0.102       - 127.0.0.1       - localhost     peerCertSANs:       - 10.0.0.100       - 10.0.0.101       - 10.0.0.102       - 127.0.0.1       - localhost networking:   podSubnet: 10.244.0.0/16   serviceSubnet: 10.96.0.0/12 ```  ### 2. Points Critiques pour Ã©viter les erreurs passÃ©es  #### A. IPs PrivÃ©es Uniquement - âœ… `advertiseAddress` : IP privÃ©e uniquement - âœ… `controlPlaneEndpoint` : IP privÃ©e uniquement - âœ… Certificats etcd : SANs avec IPs privÃ©es uniquement - âŒ Ne JAMAIS utiliser d'IP publique dans la config initiale  #### B. Configuration DNS avant bootstrap ```bash # Fix /etc/resolv.conf AVANT kubeadm init echo nameserver 1.1.1.1 > /etc/resolv.conf systemctl stop systemd-resolved systemctl disable systemd-resolved ```  #### C. Attendre entre chaque Ã©tape - AprÃ¨s `kubeadm init` : attendre que l'API server rÃ©ponde (port 6443) - AprÃ¨s chaque `kubeadm join --control-plane` : attendre que etcd cluster soit stable - VÃ©rifier avec `etcdctl endpoint health` avant de continuer  #### D. Configuration etcd cluster - Le premier master initialise etcd en mode standalone - Les autres masters rejoignent le cluster etcd existant - `initial-cluster-state=new` uniquement pour le premier master - `initial-cluster-state=existing` pour les autres masters (automatique avec kubeadm)  ## ðŸš« Erreurs Ã  Ã‰viter  1. **Ne pas initialiser les 3 masters en parallÃ¨le**    - Risque de conflits etcd    - Certificats non synchronisÃ©s  2. **Ne pas mÃ©langer IPs publiques et privÃ©es**    - ProblÃ¨mes de certificats TLS    - Connexions etcd qui Ã©chouent  3. **Ne pas oublier de configurer DNS avant bootstrap**    - Timeouts lors de la synchronisation des caches API server    - Erreurs Nameserver limits exceeded  4. **Ne pas patcher les manifests avant que le cluster soit stable**    - Attendre que tous les masters soient prÃªts    - VÃ©rifier l'Ã©tat du cluster avant modifications  ## ðŸ“ Script d'Initialisation RecommandÃ©  ```bash #!/bin/bash # 1. Master-01 init kubeadm init --config=kubeadm-config.yaml  # 2. Attendre API server until curl -k https://10.0.0.100:6443/healthz; do sleep 5; done  # 3. Master-02 join kubeadm join --config=kubeadm-config-master02.yaml --control-plane  # 4. Attendre etcd cluster stable until etcdctl --endpoints=https://10.0.0.100:2379 endpoint health; do sleep 5; done  # 5. Master-03 join kubeadm join --config=kubeadm-config-master03.yaml --control-plane  # 6. Workers (parallÃ¨le OK) for worker in worker-01 worker-02 worker-03 worker-04 worker-05; do   kubeadm join --token=XXX --discovery-token-ca-cert-hash=sha256:XXX 10.0.0.100:6443 & done wait ```  ## ðŸ” VÃ©rifications Ã  Faire  ### AprÃ¨s master-01 init - [ ] API server rÃ©pond sur https://10.0.0.100:6443 - [ ] etcd pod est Running - [ ] kube-controller-manager pod est Running - [ ] kube-scheduler pod est Running  ### AprÃ¨s chaque master join - [ ] etcd endpoint health OK - [ ] etcd cluster a le bon nombre de membres (1, puis 2, puis 3) - [ ] Pas d'erreurs dans les logs etcd/kube-apiserver  ### AprÃ¨s tous les masters - [ ] `kubectl get nodes` montre 3 masters Ready - [ ] `etcdctl endpoint status --cluster` montre 3 membres - [ ] Pas de pods en CrashLoopBackOff dans kube-system 
